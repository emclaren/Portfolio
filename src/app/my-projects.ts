import { Project } from './project';

export const PROJECTS: Project[] = [


{ id: 1, 
	name: 'Mind-Full',
  thumbname: 'Neurofeedback',
  class: 'mind-full',
  thumbnail:'assets/img/mindfull/mind-full-thumbnail-min.jpg',
  summary:'A series of games that use EEG neurofeedback to help children practice self-regulation.' ,
  overview: 'A project to redesign and evaluate Mind-Full neurofeedback games for children.',
  team: 'Elgin-Skye McLaren, Alissa Antle, Perry Tan, Srilekha Sridharan, Fan Lin, Emily Cramer',
  tags: 'NeuroSky Mindwave Headset, Unity Game Engine, Android Development',
  role: 'Project Manager, Tester, Researcher – details below',
  challengeHeading:'Adapting an existing tool for new users.',
  c1:'Children in the US and Canada are more likely to suffer from anxiety and attentional challenges than any other mental health issue. These children often have difficulty staying calm and focused at school. The Mind-Full application measures brain activity and provides feedback using a mobile application. It helps children learn to calm down and focus so they can do it on their own.',
  c2:'Mind-Full was developed for traumatized children in Nepal who struggled in school. Our work showed promise – students who used Mind-Full were better able to focus in class. We wondered: Could this technology be adapted to help North American children with anxiety and attentional challenges?',
  c3: 'To find out, our team needed to redesign and re-evaluate our existing prototype.',
  c4: '',
  problemHeading1:'Designing new interfaces',
  problemHeading2:'Evaluating the system',
  p1:'In consultation with two communities, we produced two new interfaces. The first was developed with a local First Nations community, and featured wildlife imagery, such as bears, ravens, and salmon. The second, for a nearby school district, featured silly characters. An in-depth analysis of the design process was recently accepted for publication in a top HCI journal – Transactions on Computer Human Interaction. ',
  p2:'We initially planned to simply reskin the Nepal prototype; however, we also wanted to improve usability. This meant multiple design iterations and new functionality, such as the ability to delete user accounts and track progress. Significant testing was required to ensure Mind-Full worked across different devices, and that headset measurements were accurate. ',
  p3:'I contributed throughout the ideation and design process, and was particularly helpful during testing and development. I worked closely with another developer to find and fix issues with the games. ',
  p4:'In Fall 2016, I ran a controlled study with 25 adults to better understand the short term-effects of the redesigned Mind-Full. We presented this research at the Brain Computer Interface Conference 2017 in Graz, Austria.',
  p5:'Most recently, in 2017, we ran a 20-week waitlist-controlled study with 28 children ages 6-8 with anxiety and attentional challenges in a local school district. For this study, I was involved in study design, data collection, and analysis. Children showed improvement, particularly related when it came to reducing anxiety (the manuscript with final results is forthcoming).',
  resultHeading1: 'Co-design with children',
  resultHeading2: 'Neurofeedback for youth',
  r1:'For my master’s research, I’m working with grade 3 students as design partners to incorporate sound into the Mind-Full system. Together, we “co-design” sounds to improve user experience. This work will contribute knowledge about the impact of audio feedback on children\'s (relaxed vs. anxious) brain states, and produce an enhanced Mind-Full system for use in future studies. Data collection will be completed in spring 2017.',
  r2:' ',
  r3:'I am currently exploring other neurofeedback system designs, with the aim of creating a neurofeedback tool for youth. At BrainHack DC 2017, I worked with a team to create a brainwave visualization tool using an Open BCI 8 channel EEG headset, Node.js, and p5.js. I improved my understanding of UDP sockets and time-series data. We presented this work at the November DataViz DC meetup.',
  
  caption1:'Redesigned Mind-Full Splash Screen',
  caption2:'A revamped game to help kids relax',
  caption3:'Brainstorming @ Interaction Design and Children Conference',
  caption4:'Mind-Full was built with Unity Game Engine',
  caption5:'Child Demonstrating How to Use Mind-Full',
  caption6:'Presenting our research at Neuroscience 2017',
  caption7:'Demo of Neurofeedback system developed @ BrainHackDC 2017',

},
{ id: 2, 
	name: 'Block Talks', 
  thumbname: 'AR for Literacy',
  class: 'block-talks',
  thumbnail:'assets/img/blocktalks/block-talks-thumbnail-min.jpg',
  summary:'A Mobile App and Toolkit for Kids Who Can\'t Read Good and Want to Do Other Stuff Good Too.',
  overview: 'An award-winning project developed from the ground up during Eduhacks Hackathon 2017.',
  team: 'Min Fan, Elgin-Skye McLaren, Uddipana Baishya, Shubhra Sarkar, and Amal Vincent',
  tags: 'Augmented Reality, Unity Game Engine, Tangible Computing, Games for Education',
  role: 'Ideation, Development, Project Pitch – details below', 
  challengeHeading:'Affordable AR for schools',
  c1:'One in every 10 children has trouble learning to read and spell. Block Talks was designed with these kids in mind. Created during the Eduhacks 24-hour hackathon 2017, our goal was to develop a low-cost augmented reality toolkit that could be used to support classroom learning. This project placed 3rd out of 66 submissions.',
  c2:'Block talks consists of a set of physical letters, coloured blocks, and an application running on a mobile device. Children place letters on the blocks to create words and sentences. Colored blocks are used to identify sentence structure (nouns, verbs, etc). The mobile application allows children to check their spelling, and receive feedback in the form of augmented reality animations and audio dictation. ',
  c3: '',
  c4: '',
  problemHeading1:'Rapid Ideation',
  problemHeading2:'Building the system',
  p1:'Our development process began with a phase of rapid ideation. We defined the following criteria: the project needed to be: 1. In line with our skill sets (VR, child-computer interaction, tangible computing); 2. Practicable (a working prototype within the time constraint); 3. Relevant (real-world application). On a whiteboard, we came up with 10 project ideas, and selected the one that best fit our criteria. ',
  p2:' Next, we focused on the necessary tools, frameworks, and potential gaps in the market of augment reality learning tools. I supported this process, and was responsible for gathering English Language curriculum ideas that would lend well to this type of tool. I also created paper prototypes of the UX design and sample sentences. ',
  p3:'',
  p4:'With ideas and sketches in mind, we built the app with items from a nearby dollar store. Items weren’t always a perfect fit – for example, the blocks were too small for young children – but they were what we had available. I helped build the physical prototype and gathered media assets (eg: the cat sprite). I recorded audio voiceovers and created a bootstrap landing page to explain our project. I pitched the project during multiple rounds of evaluation and during the final presentation to the judges. I learned many new skills, and some of the unique challenges of AR. For example, the change in lighting from the development area to the showcase area made object recognition more difficult with the tablet, and necessitated additional lights. Overall, the project was a success (we placed in the top 5% of submissions).',
  p5:'',
  resultHeading1: 'Iterating on the design & user testing',
  resultHeading2:'',
  r1:'We are currently in the process of developing a learning curriculum to accompany the application. In 2018, we will begin usability testing with children. We are also working on an improved feedback mechanism (see gif, below) to help guide children to the correct answer. We have detailed this project in a short publication (currently under review). ',
  r2:'',
  r3:'',
  
  caption1:'Block Talks Logo',
  caption2:'Users can scan the blocks to get feedback from the app',
  caption3:'Assembling the prototype at Eduhacks 2017',
  caption4:'Augmented reality feedback of the sentence',
  caption5:'Finalist Presentation at the Eduhacks Hackathon',
  caption6:'Block talks placed 3rd out of 66 entries.',
  caption7:'Proposed feedback design for next version of the app',
  
},

{ id: 3, 
  name: 'Audio-Matic',
  thumbname: 'AI Music',
  class: 'audio-matic',
  thumbnail:'assets/img/audiomatic/audio-matic-thumbnail-min.jpg',
  summary:'A novel system for generative music interaction',
  overview: 'Graduate coursework project for Artificial Intelligence in Computational Art and Design',
  team: 'Contributions by Dr. Steve Dipaolo',
  tags: 'Deep Learning (Magenta/TensorFlow), MaxMSP, Arduino Microcontroller, Hardware, Sound, Sockets, Tangible Computing',
  role: 'Designer, Developer',
  challengeHeading:'Making complex AI accessible',
  c1:'Artificial intelligence (AI) is everywhere. Yet many users are apprehensive of AI systems and unaware of how they work. Ethicists recommend users interact with, and reflect on, AI applications. With this in mind, I built an interactive generative music system, Audio-Matic, as a novel means of encouraging playful, accessible experiences with AI technology.',
  c2:'Users interact with Audio-Matic through a piano keyboard. The system responds by creating new melodies based on user input. These melodies are generated using Google’s deep learning tool Magenta trained on a midi corpus. The generated response is played back for users through a series of servo motors mounted on a stringed instrument.',
  c3: '',
  c4: '',
  problemHeading1:'Exploring and Building',
  problemHeading2:'',
  p1:'For this graduate-level AI course, I wanted to learn more about Deep Learning. I was curious about using these tools for generative music. After researching on different Deep Learning tools, I designed a project using Magenta – a Google Brain initiative that uses TensorFlow to create art and music. ',
  p2:'Using Magenta, I gathered a corpus of MIDI files on which to train the system. Next, I used MaxMSP multimedia software to create customized call and response sounds. Finally, I used OSC to send the signals live to an Arduino microcontroller. The microcontroller uses servo motors to produce real sound on real instruments. Working with physical instruments was a challenge. For example, I developed a prototype that used mallets to hit a xylophone. But the motors couldn’t recoil fast enough, and I was left with a faint sound. In the end, I tried running the system on a ukulele. The strings are far enough apart that a servo can play a single note at a time.  The prototype works quite well. It does not yet play a full scale range of notes but the system could eventually be scaled up to a larger stringed instrument, such as a harp. ',
  p3: '',
  p4: '',
  p5:'',
  
  r1:'A paper documenting the Audio-Matic system can be viewed online. Future plans include a larger, more robust system for public presentation. ',
  r2:'',
  r3: '',
  rImage4Class:'none',
  rImage5Class:'none',
  rImage6Class:'none',
  resultHeading1: 'Public Presentation',
  resultHeading2: '',
  caption1:'Audio-Matic Logo',
  caption2:'',
  caption3:'Sounds were created using MaxMSP',
  caption4:'',
  caption5:'Finalist Presentation at the Eduhacks Hackathon',
  caption6:'',
  caption7:'Feedback mechanism demonstration for next version of the app',

},
{ id: 4, 
  name: 'Noise Makers', 
  thumbname: 'DIY Instruments',
  class: 'tangibles',
  thumbnail:'assets/img/tangibles/tangibles-thumbnail-min.jpg',
  summary:'A series of DIY music controllers to reduce kids’ barriers to musical performance',
  overview: 'Inspired by my volunteer work with Rock Camp for Girls, I designed a workshop and series of interactive instruments to help kids learn about sound. ',
  team: 'Contributions from kids at Rock Camp for Girls Montreal & New York',
  tags: 'Microcontrollers, NodeJS, Sensors, Tangible Computing',
  role: 'Designer, Developer',
  challengeHeading:'Overcoming Stage-Fright',
  c1:'Conventional wisdom holds that music reduces stress. Research suggests a gendered effect – girls are more likely to use music as a coping mechanism than boys. However, women are less likely to play in an extracurricular band and more likely to suffer from music performance anxiety. ',
  c2:'As a musician and volunteer at Rock Camp for Girls, I noticed many girls become self-conscious about music performance during late childhood. This is often rooted in feelings of being judged on their musical skill and on the quality of their artistic output. To encourage kids to explore musical expression in a low-risk fashion, I started building DIY musical instruments.  I call these customizable controllers Noise Makers and designed a workshop to show teens how they work.',
  c3: '',
  c4: '',
  problemHeading1:'Using Sensors for Sounds',
  problemHeading2:'',
  p1:'My goal was to use affordable, open-sourced technologies to create interactive instruments that are fun and easy to use. The instruments consist of sensors (e.g.: gyroscopes, photo-sensors, ultrasonic sensors), mapped to synth instruments in MaxMSP or drum beats in Ableton Live. The sensors connect to the computer using microcontrollers such as Arduino Unos, Makey Makeys, and Lilypads for wearable sensors. For example, one instrument uses a distance sensor to control the pitch of an audio clip. As the user approaches the sensor, the pitch increases, allowing the user to play with the sound using their body. I presented this work last year at a tech meetup in a presentation called "Intro to Mad MaxMSP: Fury Code." ',
  p2:'Developing appropriate user interactions with sensors was a unique challenge and required frequent adjustments. This project was an opportunity to develop my sewing, soldering, and coding skills. I teach these and related skills as a teaching assistant for a graduate-level course in Tangible Computing at SFU and as a hardware mentor at Vancouver’s NodeSchool. ',
  p3: '',
  p4: '',
  p5:'',
  
  resultHeading1:'Kids Rock Workshop',
  resultHeading2: '',
  r1:' I have developed a curriculum for a 4-hour workshop, which I plan to lead at a 2018 edition of Rock Camp for Girls. Participant outcomes include knowledge of programming basics, understanding of hardware (sensors, microcontrollers, speakers), and customized instruments that campers can incorporate into their final performance.  ',
  r2:'',
  r3: '',
  
  caption1:'Rock Camp For Girls Showcase @ La Sala Rosa in Montreal',
  caption2:'The Pearls - practicing for their performance',
  caption3:'Instrument prototyping',
  caption4:'Audio sample pitch rises as hand gets closer',
  caption5:'Incoming sensor data connected to Nodejs Johnny-Five API',
  caption6:'Tapping the fruit changes the speed of a drum beat'



},
{ id: 5, 
  name: 'Asking for a Friend',
  thumbname: 'Health Chatbot',
  class: 'asking-for-a-friend', 
  thumbnail:'assets/img/asking/asking-for-a-friend-thumbnail-min.jpg',
  summary:'A chatbot to improve access to local mental health resources.',
  overview: 'A project created for the 2017 Lumohacks Health Hackathon. ',
  team: 'Vivian Pan, Joanna Zhao, Natasha Caton',
  tags: 'NodeJS, Microsoft Azure Bot Framework',
  role: 'Ideation, Development',

  challengeHeading:'Barriers to Seeking Help',
  c1:'Many young people experience barriers to seeking help for mental illness. These include absence of a confidante, shame/fear/stigma, and lack of knowledge about available resources. Asking for a Friend is a chatbot that improves access to nearby resources – both in-person and on-the-phone. ',
  c2:'',
  c3: '',
  c4: '',
  problemHeading1:'Using Tech for Human Connection',
  problemHeading2:'',
  p1:'We wanted to create a project to improve the lives of youth with mental health challenges. We began our ideation phase by consulting a group of first-year students with firsthand experience with mental health challenges. They flagged how difficult it is to find nearby resources and how hard it is to reach out. We conceived a chatbot that helps users connect with support organizations. ',
  p2:'Vivian Pan and I coded the bot’s back end and the web interface. Our teammates designed the conversation flow. We used Microsoft Azure’s bot service. It’s programed in Nodejs, has thorough documentation, and is easily be connected to social media platforms such as Facebook messenger. Further, the Azure bot service features the LUIS natural language processing tool to enable more flexible text recognition and improve user interaction. ',
  p3: 'During the hackathon, we successfully created a functioning prototype. The chatbot finds out what kind of help the user is looking for, and guides them to Vancouver-based services. ',
  p4: '',
  p5:'',
  rImage1:'assets/img/asking/asking-screen.jpg',
  rImage2:'assets/img/asking/asking-notepad.jpg',
  rImage3:'assets/img/blocktalks/block-talks-feedback.gif',
  rImage1Class:'none ',
  rImage2Class:'none',
  rImage3Class:'none',
  resultHeading1:'Exploring Natural Language Processing',
  resultHeading2: '',
  r1:'We received excellent feedback from the Lumohacks judges and mentors on our project. I am interested in further exploring how advanced natural language processing and A/B testing can further improve user experience. ',
  r2:'',
  r3: '',
  rImage4:'',
  rImage5:'',
  rImage6:'',
  rImage4Class:'none',
  rImage5Class:'none',
  rImage6Class:'none',
  caption1:'Programming in Nodejs',
  caption2:'',
  caption3:'Brainstorming ideas at Lumohacks Hackathon 2017',
  caption4:'Web Application Interface',
  caption5:'1AM and still coding',
  caption6:'',
  caption7:'',

  

},
{ id: 6, 
  name: 'Virtual Earthgazing',
  thumbname: 'VR Space Travel',
  class: 'earthgazing',
  thumbnail:'assets/img/earthgazement/earthgazing-thumbnail-min.jpg',
  summary:'Can VR help people feel more connected to the Earth?',
  overview: 'A research project to investigate the potential of VR to enable individuals to experience the overview effect - an awe-inspiring experience of seeing Earth from space.',
  team: 'SIAT\'s iSpace lab - Bernard Riecke, Katerina Stepanova, Denise Quesnel, Alex Kitson, Mirjana Prpa, Ivan Aguilar, Elgin-Skye McLaren',
  tags: 'Virtual Reality, HTC Vive, Unity Game Engine',
  role: 'Researcher - details below',

  challengeHeading:'',
  c1:'Astronauts often report a shift in worldview upon seeing Earth from space. The awe-inspired recognition of our planet as a fragile, and deeply connected place, is referred to as the “overview effect.” This project stemmed from the question of whether we could simulate this shift in awareness using virtual reality technology.',
  c2:'',
  c3: '',
  c4: '',

  problemHeading1:'Measuring Awe',
  problemHeading2:'',
  p1:'I worked with members of the iSpace team to lead a research study in late 2017. I contributed to experimental design, study logistics, equipment prep, and also worked with participants. As researcher, I welcomed participants, asked them to complete a pre-study survey, and set them up in the VR equipment. We used an HTC Vive, a custom-built chair for navigating the environment, biosensors, and a camera to record and measure the onset of goosebumps. Once the user was prepared, we ran a 15-minute, custom VR experience that simulated going into space. Following the experience, I led qualitative, cued-recall interviews to discover insights about how users felt over the course of the activity. In particular, we determined which moments participants found most memorable, whether they experienced “awe”, and documented usability issues within the game.  ',
  p2:'',
  p3: '',
  p4: '',
  p5:'',
  rImage1:'',
  rImage2:'',
  rImage3:'',
  rImage1Class:'none',
  rImage2Class:'none',
  rImage3Class:'none',

  rImage4:'',
  rImage5:'',
  rImage6:'',
  rImage4Class:'none',
  rImage5Class:'none',
  rImage6Class:'none',
  resultHeading1:'Continued Research',
  resultHeading2: '',
  r1:'Data analysis is ongoing, and many users reported a shift in awareness about the Earth after the VR experience – similar to what astronauts report. I will be assisting with future research studies, such as partnership with a local museum and public exhibitions in 2018.  ',
  r2:'',
  r3: '',
  caption1:'Capturing the feeling of seeing Earth from space',
  caption2:'',
  caption3:'The experience was run using HTC Vive',
  caption4:'',
  caption5:'Simulated VR view',
  caption6:'',
  caption7:'',
}
];
