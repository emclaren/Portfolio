import { Project } from './project';

export const PROJECTS: Project[] = [

{ id: 1, 
	name: 'Mind-Full',
  thumbname: 'Neurofeedback',
  class: 'mind-full',
  thumbnail:'assets/img/mindfull/mind-full-thumbnail-min.jpg',
  summary:'A neurofeedback application to help children learn to self-regulate.',
  overview: 'Mind-Full is a series of games that use EEG-technology to help children (ages 6-10) practice self-regulation.' ,
  team: 'Alissa N. Antle, Perry Tan, Srilekha Kirsh Sridharan, Fan Lin, Emily Cramer',
  tags: 'NeuroSky Mindwave Mobile Headset, Unity Game Engine, Android Development',
  role: 'Developer, Tester, Project Manager, Researcher – further details below',
  challengeHeading:'Adapting an existing tool for new users.',
  c1:'North American children are more likely to suffer from anxiety and attentional challenges than any other mental health issue. These children often have difficulty staying calm and attentive at school. Mind-Full measures brain activity and provides feedback using a mobile application. It helps children learn to calm down and focus, so they can do it on their own. ',
  c2:'Mind-Full was initially developed for children at a school in Nepal who had experienced trauma. Results from our earlier work with these children showed improvements in their ability to pay attention during class. We wondered, could this technology be adapted to help North American children with anxiety and attentional challenges?',
  c3: 'In order to explore this research question, our team needed to first redesign and re-evaluate the existing prototype. ',
  c4: '',
  problemHeading1:'Designing new interfaces',
  problemHeading2:'Evaluating the system',
  p1:'The initial Mind-Full prototype consisted of three games, two for relaxation, and one for attention. The imagery in these games was specific to Nepal. Although North American children could use these games, we recognized they may not be as relatable or engaging for western children.  ',
  p2:'In consultation with local communities interested in working with us on  research studies, we developed two new Mind-Full user interfaces. The first was developed with a local First Nations community, and featured imagery of wildlife, such as bears, ravens and salmon. The second, developed with a nearby school district, featured silly characters, such as a squirrel and a sasquatch. An in-depth analysis of this design process has been accepted for publication in a top journal – Transactions on Computer Human Interaction. ',
  p3:'While we planned to simply reskin the prototype, redesigning the new application was more complex than expected. In order to create an application that was intuitive to use, we had to iterate on the design, and add new functionality (for example, the ability to delete users). Significant testing was required to ensure it responded consistently on different devices, and headset data was measured correctly. I contributed to the ideation and design of the new games, and worked with Perry Tan to develop, and test the games. ',
  p4:'In 2017, we ran a 20-week study with 28 children ages 6-8 with anxiety and attentional challenges in a local school district. For this study, I was involved in data collection, analysis, and analysis of the results. Results of this study are still forthcoming. In Fall 2016, I ran a controlled study with 25 adults, to better understand the short term-effects of using Mind-Full. We presented this research at the Brain Computer Interface Conference 2017 in Graz, Austria. ',
  resultHeading1: 'Co-design with children',
  resultHeading2: 'Neurofeedback for youth',
  r1:'Although we have not yet completed the analysis of our most recent study with children, many reported that they wished that the game had sound. For my master’s research, I’m working with grade 3 students as design partners to incorporate sound into the Mind-Full system. My data collection will be completed by February 2017. This work will contribute knowledge about the impact of audio feedback on children\'s (relaxed vs. anxious) brain states, and produce an enhanced Mind-Full system for use in future studies.',
  r2:' ',
  r3:'I am currently exploring other neurofeedback system designs, with the aim of creating a neurofeedback tool for youth. At BrainHack DC 2017, I worked with a team to create a brainwave visualization tool using an Open BCI 8 channel EEG headset, Node.js, and p5.js. This allowed me to further develop my understanding of networking, and time-series data. We presented this work at the November DataViz DC meetup. ',
  


  rImage1:'',
  rImage2:'',
  rImage3:'',
  rImage1Class:'none',
  rImage2Class:'none',
  rImage3Class:'none',
  rImage4:'assets/img/mindfull/brainhack-min.jpg',
  rImage5:'assets/img/mindfull/brainhack2-min.jpg',
  rImage6:'assets/img/mindfull/brainhack-close-min.png',
  rImage4Class:'col-xs-12 col-sm-4 col-lg-6',
  rImage5Class:'none',
  rImage6Class:'col-xs-12 col-sm-4 col-lg-6'
},
{ id: 2, 
	name: 'Block Talks', 
  thumbname: 'AR for Literacy',
  class: 'block-talks',
  thumbnail:'assets/img/blocktalks/block-talks-thumbnail-min.jpg',
  summary:'A mobile application to help children learn to read.',
  overview: 'Block talks is a tangible Augmented Reality toolkit to help kids ages 7-9 learn to read.',
  team: 'Uddipana Baishya, Min Fan, Shubhra Sarkar, and Amal Vincent',
  tags: 'Augmented Reality, Unity Game Engine, Tangible, Games for Education',
  role: 'Ideation, Development, Project Pitch – further details below', 

  challengeHeading:'Affordable AR for schools',
  c1:'Over 10% of children have trouble learning to read and spell. Block talks was designed with these kids in mind.',
  c2:'Block Talks was created developed during the Eduhacks 24-hour hackathon 2017. Our goal was to develop a low-cost augmented reality tool that could be used in a classroom. Block talks consists of a set of physical letters, coloured blocks, and an application running on a mobile device. Children place letters on the blocks to make words and sentences. They can use the application to check their spelling, and receive augmented reality and audio feedback from the tablet application. This project came in 3rd place out of 66 submissions.',
  c3: '',
  c4: '',


  problemHeading1:'Rapid Ideation',
  problemHeading2:'Building the system',
 
  p1:'Before we began developing our prototype, we completed a rapid ideation phase. We started off by defining the following criteria – the project needed to be 1) Related to our collective research interests – VR, children, tangibles; 2) Practicable –we could build a working prototype within the time constraint. 3) Useful – there had to be a real world application for our project. We quickly came up with 8 project ideas, and then chose Block Talks because it was the best fit within our criteria. Next, we started researching the necessary tools, and doing a quick survey of augmented reality apps for learning. I was responsible for finding example English Language curriculum ideas that could work well within the context of the prototype.',
  p2:' ',
  p3:'',
  p4:'Once we had an idea in mind, we started building physical prototypes using items from a nearby dollar store. Although they were not exactly what we wanted (for example, the blocks were too small for young children), we used them regardless so we could quickly get our ideas into the real world. I assisted with building the physical prototype and gathering media assets for the prototype (eg: the cat sprite). I recorded audio voiceovers, and created a bootstrap landing page to explain our project. I pitched the project during multiple rounds of evaluation, and completed the final presentation to the judges. I learned many of the challenges with building AR applications (eg: lighting).',

  resultHeading1: 'Iterating on the design & user testing',
  resultHeading2:'',

  r1:'We are currently in the process of developing a learning curriculum to accompany the application and testing improved feedback mechanism (see below). We intend to begin usability testing in the spring to explore children’s ability to use this type of system. We have a work-in-progress paper currently under review.  ',
  r2:'',
  r3:'',

  rImage1:'assets/img/blocktalks/block-talks-presentation-min.jpg',
  rImage2:'assets/img/blocktalks/blocktalks3-min.jpg',
  rImage3:'assets/img/blocktalks/block-talks-feedback.gif',
  rImage1Class:'none',
  rImage2Class:'none',
  rImage3Class:'none ',

  rImage4:'assets/img/blocktalks/block-talks-feedback.gif',
  rImage5:'',
  rImage6:'',
  rImage4Class:'col-xs-12 col-sm-6 col-lg-12 ',
  rImage5Class:'none',
  rImage6Class:'none',
  
},
{ id: 3, 

  name: 'Audio-Matic',
  thumbname: 'AI Music',
  class: 'audio-matic',
  thumbnail:'assets/img/audiomatic/audio-matic-thumbnail-min.jpg',
  summary:'A novel system for generative music interaction',
  overview: 'Audio-Matic is a novel means of encouraging playful, accessible experiences with AI technology.',
  team: 'Contributions by Dr. Steve Dipaolo',
  tags: 'Deep Learning (Magenta/TensorFlow), MaxMSP, Arduino Microcontroller, Hardware, Sound, Sockets, Tangibles',
  role: 'Designer, Developer',

  challengeHeading:'Making complex AI accessible',
  c1:'Artificial intelligence (AI) is increasingly ubiquitous. Yet many remain apprehensive of AI systems, or are unaware of how they work. Ethicists recommend users interact with, and reflect on, AI applications. With this in mind, I propose an interactive generative music system, Audio-Matic, as a novel means of encouraging playful, accessible experiences with AI technology.',
  c2:'Users interact with Audio-Matic through a piano keyboard.  The system responds by creating new melodies based on user input. These melodies are generated using Google’s deep learning tool Magenta trained on a midi corpus. The generated response is played back for users through a series of servo motors mounted on a stringed instrument.',
  c3: '',
  c4: '',




  problemHeading1:'Creating the System',
  problemHeading2:'Evaluating the system',
  p1:'This work was created for the course Iat 813 - Artificial Intelligence. I wanted to get my hands dirty working with Deep Learning and Audio. I was curious about Magenta. First I got magenta set up and working on my computer, I trained it on my own corpus. Next, I connected it to Max MSP which allowed me to customize the call and response sounds. I connected Max MSP to an Arduino board, and started small. powering LEDs etc using the Max MSP output. The most challenging part was creating sound on a physical object.I wanted to use a glockenschpeil & a series of 14 hammers, however, I struggled to get more than a small sound from the device. I had better luck conencting servo motors to a stringed instrument, but because stringed instruments such as the guitar are lacking notes, the final project doesn\'t produce a full range of notes. ',
  p2:'',
  p3: '',
  p4: '',


  rImage1:'assets/img/audiomatic/audiomaticgif.gif',
  rImage2:'assets/img/blocktalks/blocktalks3-min.jpg',
  rImage3:'assets/img/blocktalks/block-talks-feedback.gif',
  rImage1Class:'none',
  rImage2Class:'none ',
  rImage3Class:'none',
  rImage4:'',
  rImage5:'',
  rImage6:'',
  r1:'A full summary of this proect can be viewed here[link here]. I am currently working on building a version with more notes included, to produce a full scale\'s worht of repsonses. I am creating an art piece with it which I will be submitting to VIVO Media Arts center.',
    r2:'',
  r3: '',
  rImage4Class:'none',
  rImage5Class:'none',
  rImage6Class:'none',
  resultHeading1: 'Co-design with children',
  resultHeading2: 'Co-design with children',

},
{ id: 4, 
  name: 'Tangible Audio', 
  thumbname: 'Tangible Audio',
  class: 'tangibles',
  thumbnail:'assets/img/tangibles/tangibles-thumbnail-min.jpg',
  summary:'Lowering the barriers for kids musical performance',
  overview: 'Tangible instruments is a series of DIY music controllers to help kids with performance anxiety feel comfortable playing music.',
  team: 'Contributions from campers at Rock Camp for Girls Montreal & New York',
  tags: 'Open source Microcontrollers, NodeJS, Sensors, Toys and Tangibles',
  role: 'Designer, Developer, UX researcher',
  challengeHeading:'Kids Rock',
  c1:' Inspired by volunteer work with kids at Rock Camp for Girls.  Conventional wisdom holds that music reduces stress. Further, evidence suggests a gendered effect – girls are more likely to use music as a coping mechanism than boys (Campbell et al., 2007 p229). However, women are less likely to play in an extracurricular band (Clawson, 1999) and more likely to suffer from music performance anxiety (Kenny and Osborne, 2006, p106; Kenny, 2011). Performance anxiety is rooted in feelings of vulnerability, often related to social evaluation (i.e. we worry what other people think). Music performance anxiety is triggered when people believe others are judging them based on their skill or quality of artistic output. For my master’s project I would like to investigate ways of using technology to reduce music performance anxiety among women and girls.',
  c2:'',
  c3: '',
  c4: '',
  problemHeading1:'A two part approach',
  problemHeading2:'Evaluating the system',
  p1:'Specifically, I propose adapting tangible computing technologies into interactive instruments that are enjoyable and easy to use. Different sounds, rhythms and melodies would be manipulated using bodily motions, movement, and dance. People are more comfortable singing karaoke than performing in a live band. Similarly, users of this tangible technology could create music with little to no training. This, in turn, could reduce the feelings of vulnerability by mitigating the threat of social evaluation and rendering performance less stressful (and even fun). My informal hypothesis is: girls can, and should, rock.',
   p2:'',
  p3: '',
  p4: '',
  rImage1:'assets/img/tangibles/arduino-node.jpg',
  rImage2:'assets/img/tangibles/girlsrock7-min.jpg',
  rImage3:'assets/img/tangibles/tangibles-thumbnail-min.jpg',
  rImage1Class:'none ',
  rImage2Class:'none ',
  rImage3Class:'none ',
  resultHeading1:'',
   resultHeading2: 'Co-design with children',
  r1:' This has been a personal project of mine for several years. Creating, and experimenting with tangible tools to make sound. This was a fun way improve my saudering, sewing and coding skills. As a result of this work, I was asked to TA for IAT 884- tangible computing in Winter 2018. I am now also a  hardware mentor for the Vancouver chapter of NodeSchool.  I presented some of my findings at the local tech meet up Code and Coffee, in a presentation called "Intro to Max MSP: Fury Code" in 2016I am currently collaborating with artists in town to add sound + sensors to public artwork. ',
   r2:'',
  r3: '',
  rImage4:'',
  rImage5:'',
  rImage6:'',
  rImage4Class:'none',
  rImage5Class:'none',
  rImage6Class:'none',

},
{ id: 5, 
  name: 'Virtual Earthgazing',
  thumbname: 'VR Earthgazing',
  class: 'earthgazing',
  thumbnail:'assets/img/earthgazement/earthgazing-thumbnail-min.jpg',
  summary:'In fall 2017 I collaborated with researcher from the iSpace lab to study the power of VR to make people feel more connected to the earth.',
  overview: 'Virtual Earthgazing is a project investigating the potential of VR to enable individuals to experience the overview effect, the awe-inspiring experience of seeing Earth from space resulting in a heightened awareness of the planet.',
  team: 'Worked with with SIAT\'s iSpace lab ( Bernard Riecke, Katerina Stepanova, Denise Quesnel, Alex Kitson, Mirjana Prpa, Ivan Aguilar',
  tags: 'Virtual Reality, HTC Vive, Unity Game Engine',
  role: 'Researcher (details below)',

  challengeHeading:'',
  c1:'"How can we use immer­sive VR to give people piv­otal pos­i­tive expe­ri­ences with­out having to send them out into space?" “We went to the Moon as tech­ni­cians, we returned as human­i­tar­i­ans” reflected Edgar Mitchell after his space flight. This describes the overview effect – a pro­found awe-inspiring expe­ri­ence of seeing Earth from space result­ing in a cog­ni­tive shift in world­view, lead­ing to a more con­scious and caring view on our planet. Experiencing Earth from space first-hand made many astro­nauts real­ize that Earth is frag­ile, with­out bor­ders, lead­ing to a feel­ing of con­nect­ed­ness to human­ity and our planet(see astro­nauts’ quotes). Such an aware­ness shift could have a pos­i­tive impact on our soci­ety and planet, espe­cially if we had a tool that allowed for more people to expe­ri­ence it with­out the risk, cost, and envi­ron­men­tal foot­print asso­ci­ated with actual space flight. To pursue this dream, the iSpace Lab inves­ti­gates how we could best use the poten­tial of immer­sive vir­tual real­ity to give people a glimpse of the overview effect with­out having to send more rock­ets to space. At the same time, we use vir­tual real­ity as a tool allow­ing us to better under­stand the expe­ri­ence and under­ly­ing trig­gers of the overview effect phenomenon.',
  c2:'',
  c3: '',
  c4: '',

  problemHeading1:'Evaluating the system',
  problemHeading2:'',
  p1:'I was involved as a researcher for a research study in 2017. I helped with planning, logistics, and running participants. Users ran the experiment for approximately 15 minutes; at which point I did cued- recall interviews to find out more about what users experienced. What were the most memorable moments, what were usability issues. ',
  p2:'',
  p3: '',
  p4: '',

  rImage1:'',
  rImage2:'',
  rImage3:'',
  rImage1Class:'none',
  rImage2Class:'none',
  rImage3Class:'none',

  rImage4:'',
  rImage5:'',
  rImage6:'',
  rImage4Class:'none',
  rImage5Class:'none',
  rImage6Class:'none',
  resultHeading1:'Ongoing Work',
  resultHeading2: 'Co-design with children',
  r1:'Qualitative User research, interview technique for evaluating VR experineces. Setting up and troubleshooting immersive VR experiences. ',
 r2:'',
  r3: '',
},
{ id: 6, 
  name: 'Asking for a Friend',
  thumbname: 'Health Chatbot',
  class: 'asking-for-a-friend', 
  thumbnail:'assets/img/asking/asking-for-a-friend-thumbnail-min.jpg',
  summary:'In fall 2017, a team of students developed a tool to improving accessibility of mental health resources for youth.',
  overview: 'Asking For a Friend is a chatbot designed to connect individuals seeking help with mental health challenges with local resources and support.',
  team: 'Vivian Pan, Joanna Zhao, Natasha Caton',
  tags: 'NodeJS, Microsoft Azure Bot Framework',
  role: 'DeIdeation, Developed front end usability, Coded the backend with Vivian Pan',

  challengeHeading:'Youth Mental Illness',
  c1:'',
  c2:'',
  c3: '',
  c4: '',
  problemHeading1:'',
  problemHeading2:'Evaluating the system',
  p1:'As a team we decided right away that we wanted to help youth with mental health issues. We had a diverse team comprised of a business student, a public health student, a computer engineer. Our criteria was that we needed the project to be 1) practicable, could we code out a prototype in 24 hours. 2) Useful, is there a need for this project. 3) Scalable, could this idea be created in a real way.  We began by doing interviews with individuals about their experiences with mental health challenges. We came up with several ideas, but they were discarded because they did not fit our pre-defined criteria.  Vivian and I worked on the chatbot, while our teamates in business and public health thought of resources we could recommend people to. ',
 p2:'',
  p3: '',
  p4: '',
  rImage1:'assets/img/asking/asking-screen.jpg',
  rImage2:'assets/img/asking/asking-notepad.jpg',
  rImage3:'assets/img/blocktalks/block-talks-feedback.gif',
  rImage1Class:'none ',
  rImage2Class:'none',
  rImage3Class:'none',
  resultHeading1:'Bot Services',
  resultHeading2: 'Co-design with children',
  r1:'During this project I learned about building chatbots. We researched the different chatbot mechanisms (watson/Alexa/Azure chatbot), and decided on Azure because it had the flexibility we were hoping for (look at their features)',
   r2:'',
  r3: '',
  rImage4:'',
  rImage5:'',
  rImage6:'',
  rImage4Class:'none',
  rImage5Class:'none',
  rImage6Class:'none',
  

}

];
